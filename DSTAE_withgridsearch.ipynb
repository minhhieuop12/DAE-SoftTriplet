{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Autoencoder với Soft Triple Loss và Grid Search\n",
        "\n",
        "Notebook này triển khai Deep Autoencoder (DAE) kết hợp với Soft Triple Loss để phát hiện bất thường trong dữ liệu mạng. Code thực hiện grid search để tìm số chiều tiềm ẩn (`latent_dims`) và số tâm (`K`) tối ưu.\n",
        "\n",
        "## Các bước chính:\n",
        "1. Tiền xử lý dữ liệu từ file CSV.\n",
        "2. Xây dựng và huấn luyện DAE với Soft Triple Loss.\n",
        "3. Grid search cho `latent_dims` ([8, 16, 32]) và `K` ([3, 5, 7, 11]).\n",
        "4. Trích xuất đặc trưng và đánh giá với các mô hình phân loại.\n",
        "5. Lưu kết quả grid search vào file markdown và vẽ biểu đồ mất mát.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import thư viện\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tiền xử lý dữ liệu\n",
        "\n",
        "Hàm `preprocess_data` thực hiện các bước:\n",
        "- Tải file CSV và kiểm tra cột `Label`.\n",
        "- Ánh xạ nhãn thành nhị phân (0: BENIGN, 1: khác).\n",
        "- Loại bỏ giá trị vô cực và NaN.\n",
        "- Loại bỏ cột không phải số và đặc trưng có phương sai thấp.\n",
        "- Chuẩn hóa dữ liệu bằng MinMaxScaler.\n",
        "- Cân bằng dữ liệu bằng RandomUnderSampler.\n",
        "- Chia tập huấn luyện và kiểm tra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_labels_to_numeric(labels):\n",
        "    label_mapping = {'BENIGN': 0}\n",
        "    numeric_labels = labels.apply(lambda x: 0 if 'BENIGN' in str(x) else 1)\n",
        "    return numeric_labels\n",
        "\n",
        "def preprocess_data(data_path, features):\n",
        "    data = pd.read_csv(data_path, low_memory=False)\n",
        "    if ' Label' not in data.columns:\n",
        "        raise ValueError(\"Cột 'Label' không tồn tại trong dữ liệu.\")\n",
        "    missing_features = [f for f in features if f not in data.columns]\n",
        "    if missing_features:\n",
        "        features = [f for f in features if f in data.columns]\n",
        "    if len(features) == 0:\n",
        "        raise ValueError(\"Không có đặc trưng nào hợp lệ trong dữ liệu.\")\n",
        "    data = data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    numeric_labels = map_labels_to_numeric(data[' Label'])\n",
        "    benign_data = data[data[' Label'].str.contains('BENIGN', case=False, na=False)][features]\n",
        "    all_data = data[features]\n",
        "    if benign_data.empty:\n",
        "        raise ValueError(\"Không tìm thấy dữ liệu hợp lệ với nhãn 'Benign'.\")\n",
        "    non_numeric_cols = benign_data.select_dtypes(exclude=['float64', 'int64']).columns\n",
        "    if len(non_numeric_cols) > 0:\n",
        "        benign_data = benign_data.drop(columns=non_numeric_cols)\n",
        "        all_data = all_data.drop(columns=non_numeric_cols)\n",
        "    benign_data = benign_data.loc[:, benign_data.var() > 1e-3]\n",
        "    all_data = all_data[benign_data.columns].dropna()\n",
        "    numeric_labels = numeric_labels[all_data.index]\n",
        "    if benign_data.empty:\n",
        "        raise ValueError(\"DataFrame rỗng sau khi loại bỏ NaN.\")\n",
        "    selected_features = benign_data.columns.tolist()\n",
        "    scaler = MinMaxScaler()\n",
        "    benign_data_scaled = scaler.fit_transform(benign_data)\n",
        "    all_data_scaled = scaler.transform(all_data)\n",
        "    train_data, test_data = train_test_split(benign_data_scaled, test_size=0.2, random_state=42)\n",
        "    rus = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
        "    all_data_resampled, test_labels_resampled = rus.fit_resample(all_data_scaled, numeric_labels)\n",
        "    return train_data, test_data, all_data_resampled, test_labels_resampled, scaler, selected_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mô hình Deep Autoencoder\n",
        "\n",
        "Lớp `DeepAutoencoder` xây dựng một autoencoder sâu với:\n",
        "- **Encoder**: Giảm chiều dữ liệu qua các tầng [512, 256, 128, latent_dim].\n",
        "- **Decoder**: Tái tạo dữ liệu từ không gian tiềm ẩn.\n",
        "- Mỗi tầng sử dụng SELU, BatchNorm, và Dropout (0.2) để ổn định huấn luyện."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(DeepAutoencoder, self).__init__()\n",
        "        hidden_dims = [512, 256, 128, latent_dim]\n",
        "        encoder_layers = []\n",
        "        prev_dim = input_dim\n",
        "        for dim in hidden_dims:\n",
        "            encoder_layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.SELU(),\n",
        "                nn.BatchNorm1d(dim),\n",
        "                nn.Dropout(0.2)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "        decoder_layers = []\n",
        "        hidden_dims = hidden_dims[::-1]\n",
        "        for dim in hidden_dims[:-1]:\n",
        "            decoder_layers.extend([\n",
        "                nn.Linear(prev_dim, dim),\n",
        "                nn.SELU(),\n",
        "                nn.BatchNorm1d(dim),\n",
        "                nn.Dropout(0.2)\n",
        "            ])\n",
        "            prev_dim = dim\n",
        "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded, encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Soft Triple Loss\n",
        "\n",
        "Hàm `soft_triple_loss` tối ưu hóa biểu diễn tiềm ẩn bằng cách sử dụng nhiều tâm (`K`) cho mỗi lớp (benign và anomalous). Các tham số mặc định:\n",
        "- `gamma=0.1`, `delta=0.01`, `lambda_param=2.0`.\n",
        "- Sử dụng chuẩn hóa L2 và softmax để tính độ tương đồng."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def soft_triple_loss(encoded, labels, centers, gamma=0.1, delta=0.01, lambda_param=2.0):\n",
        "    if len(encoded) == 0 or centers is None:\n",
        "        return torch.tensor(0.0, device=encoded.device)\n",
        "    \n",
        "    K = centers.size(1)\n",
        "    num_classes = 2\n",
        "    d = encoded.shape[1]\n",
        "    \n",
        "    encoded_norm = F.normalize(encoded, p=2, dim=1)\n",
        "    centers_norm = F.normalize(centers, p=2, dim=2)\n",
        "    inner_logits = torch.einsum('bd,nkd->bnk', encoded_norm, centers_norm)\n",
        "    inner_softmax = F.softmax(inner_logits / gamma, dim=2)\n",
        "    S = lambda_param * (torch.sum(inner_softmax * inner_logits, dim=2) - delta * labels)\n",
        "    outer_softmax = F.softmax(S, dim=1)\n",
        "    loss = -torch.sum(torch.log(torch.sum(outer_softmax * labels, dim=1) + 1e-6))\n",
        "    return loss / encoded.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hàm hỗ trợ\n",
        "\n",
        "- `initialize_centers`: Khởi tạo tâm ngẫu nhiên cho Soft Triple Loss.\n",
        "- `train_DAE_soft_triplet`: Huấn luyện DAE với Soft Triple Loss, sử dụng early stopping và scheduler.\n",
        "- `extract_features`: Trích xuất đặc trưng từ không gian tiềm ẩn.\n",
        "- `evaluate_model`: Đánh giá mô hình phân loại với các chỉ số Precision, Recall, F1-score, và Confusion Matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_centers(d, num_classes=2, K=5):\n",
        "    return torch.randn(num_classes, K, d, requires_grad=True)\n",
        "\n",
        "def train_DAE_soft_triplet(model, train_loader, optimizer, scheduler, centers, epochs=30, device='cuda', patience=10, gamma=0.1, delta=0.01, lambda_param=2.0):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "        for data, labels in train_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
        "            output, encoded = model(data)\n",
        "            recon_loss = nn.MSELoss()(data, output)\n",
        "            soft_triple = soft_triple_loss(encoded, labels_one_hot, centers, gamma, delta, lambda_param)\n",
        "            total_loss = recon_loss + soft_triple\n",
        "            if torch.isnan(total_loss):\n",
        "                continue\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            epoch_loss += total_loss.item()\n",
        "            batch_count += 1\n",
        "        if batch_count == 0:\n",
        "            continue\n",
        "        avg_loss = epoch_loss / batch_count\n",
        "        losses.append(avg_loss)\n",
        "        scheduler.step(avg_loss)\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
        "        if not torch.isnan(torch.tensor(avg_loss)) and avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'best_DAE_soft_triplet_latent_{model.encoder[-4].out_features}_K_{centers.size(1)}.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "    return losses\n",
        "\n",
        "def extract_features(model, data, device='cuda'):\n",
        "    model.eval()\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        _, encoded = model(data_tensor)\n",
        "    return encoded.cpu().numpy()\n",
        "\n",
        "def evaluate_model(model, model_name, data, labels):\n",
        "    start_time = time.time()\n",
        "    if model_name == 'Linear Regression':\n",
        "        predictions_proba = model.predict(data)\n",
        "        predictions = (predictions_proba >= 0.5).astype(int)\n",
        "    else:\n",
        "        predictions = model.predict(data)\n",
        "    training_time = time.time() - start_time\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    precision = precision_score(labels, predictions, zero_division=0)\n",
        "    recall = recall_score(labels, predictions, zero_division=0)\n",
        "    f1 = f1_score(labels, predictions, zero_division=0)\n",
        "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
        "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
        "    print(f\"{model_name} - F1-score: {f1:.4f}\")\n",
        "    print(f\"{model_name} - Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"{model_name} - Số lượng bất thường phát hiện: {np.sum(predictions)}\")\n",
        "    print(f\"{model_name} - Training time: {training_time:.2f} seconds\")\n",
        "    return precision, recall, f1, training_time, cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hàm chính với Grid Search\n",
        "\n",
        "Hàm `main` thực hiện:\n",
        "- Tiền xử lý dữ liệu.\n",
        "- Grid search cho `latent_dims=[8, 16, 32]` và `K=[3, 5, 7, 11]`.\n",
        "- Huấn luyện DAE với Soft Triple Loss cho từng tổ hợp.\n",
        "- Trích xuất đặc trưng và đánh giá với các mô hình phân loại.\n",
        "- Lưu kết quả vào file markdown và vẽ biểu đồ mất mát."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    data_path = r'C:\\Users\\belon\\Downloads\\combine.csv\\combine.csv'\n",
        "    df = pd.read_csv(data_path)\n",
        "    features = df.columns.drop([' Label', ' Destination Port'])\n",
        "    train_data, test_data, all_data_resampled, test_labels_resampled, scaler, selected_features = preprocess_data(data_path, features)\n",
        "    if len(train_data) == 0 or len(test_data) == 0:\n",
        "        raise ValueError(\"Dữ liệu huấn luyện hoặc kiểm tra rỗng.\")\n",
        "    \n",
        "    input_dim = len(selected_features)\n",
        "    all_data_tensor = torch.tensor(all_data_resampled, dtype=torch.float32)\n",
        "    labels_tensor = torch.tensor(test_labels_resampled.values, dtype=torch.long)\n",
        "    dataset_all = torch.utils.data.TensorDataset(all_data_tensor, labels_tensor)\n",
        "    train_loader_all = torch.utils.data.DataLoader(dataset_all, batch_size=2048, shuffle=True, num_workers=4)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
        "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "        'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
        "    }\n",
        "    \n",
        "    # Grid Search parameters\n",
        "    latent_dims = [8, 16, 32]\n",
        "    K_values = [3, 5, 7, 11]\n",
        "    results = []\n",
        "    \n",
        "    for latent_dim, K in product(latent_dims, K_values):\n",
        "        print(f\"\\nTraining DAE with Soft Triple Loss (latent_dim={latent_dim}, K={K})...\")\n",
        "        DAE_soft_triplet = DeepAutoencoder(input_dim=input_dim, latent_dim=latent_dim).to(device)\n",
        "        centers = initialize_centers(d=latent_dim, num_classes=2, K=K).to(device)\n",
        "        optimizer = torch.optim.Adam(list(DAE_soft_triplet.parameters()) + [centers], lr=0.05)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        losses = train_DAE_soft_triplet(\n",
        "            DAE_soft_triplet, train_loader_all, optimizer, scheduler, centers, \n",
        "            epochs=30, device=device, gamma=0.1, delta=0.01, lambda_param=2.0\n",
        "        )\n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        plt.plot(losses)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Total Loss')\n",
        "        plt.title(f'DAE with Soft Triple Loss (latent_dim={latent_dim}, K={K})')\n",
        "        plt.savefig(f'loss_plot_latent_{latent_dim}_K_{K}.png')\n",
        "        plt.close()\n",
        "        \n",
        "        model_path = f'best_DAE_soft_triplet_latent_{latent_dim}_K_{K}.pth'\n",
        "        if os.path.exists(model_path):\n",
        "            DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        \n",
        "        soft_triplet_features = extract_features(DAE_soft_triplet, all_data_resampled, device)\n",
        "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
        "            soft_triplet_features, test_labels_resampled, test_size=0.2, random_state=42, stratify=test_labels_resampled\n",
        "        )\n",
        "        \n",
        "        for model_name, model in models.items():\n",
        "            print(f\"\\nEvaluating {model_name} with latent_dim={latent_dim}, K={K}...\")\n",
        "            start_time = time.time()\n",
        "            model.fit(train_features, train_labels)\n",
        "            eval_time = time.time() - start_time\n",
        "            precision, recall, f1, _, cm = evaluate_model(model, model_name, test_features, test_labels)\n",
        "            results.append({\n",
        "                'Latent Dim': latent_dim,\n",
        "                'K': K,\n",
        "                'Model': model_name,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1-score': f1,\n",
        "                'Training Time (DAE)': training_time,\n",
        "                'Training Time (Model)': eval_time\n",
        "            })\n",
        "    \n",
        "    # Lưu kết quả grid search\n",
        "    results_df = pd.DataFrame(results)\n",
        "    for model_name in models.keys():\n",
        "        model_results = results_df[results_df['Model'] == model_name]\n",
        "        markdown_table = f\"# Kết quả Grid Search cho {model_name}\\n\\n\"\n",
        "        markdown_table += \"| Latent Dim | K | Precision | Recall | F1-score | Thời gian huấn luyện DAE (s) | Thời gian huấn luyện Model (s) |\\n\"\n",
        "        markdown_table += \"|------------|---|-----------|--------|----------|-----------------------------|-------------------------------|\\n\"\n",
        "        for _, row in model_results.iterrows():\n",
        "            markdown_table += f\"| {row['Latent Dim']} | {row['K']} | {row['Precision']:.4f} | {row['Recall']:.4f} | {row['F1-score']:.4f} | {row['Training Time (DAE)']:.2f} | {row['Training Time (Model)']:.2f} |\\n\"\n",
        "        print(markdown_table)\n",
        "        with open(f'grid_search_results_{model_name.lower().replace(\" \", \"_\")}.md', 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_table)\n",
        "    \n",
        "    return results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chạy chương trình\n",
        "\n",
        "Chạy hàm `main` để thực hiện toàn bộ quy trình. Đảm bảo file `combine.csv` đã có sẵn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(data_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=8, K=3)...\n",
            "Epoch [1/30], Total Loss: 0.2652, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1362, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1223, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1136, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1100, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.1056, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.1044, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.1009, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0998, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0961, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0943, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0956, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0923, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0936, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0910, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.5876, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.1098, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.1002, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0955, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0937, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0943, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0918, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.2336, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0987, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0949, LR: 0.050000\n",
            "Early stopping at epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=8, K=3...\n",
            "Linear Regression - Precision: 0.9668\n",
            "Linear Regression - Recall: 0.9954\n",
            "Linear Regression - F1-score: 0.9809\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[104417   3693]\n",
            " [   495 107614]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 111307\n",
            "Linear Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=8, K=3...\n",
            "Gradient Boosting - Precision: 0.9831\n",
            "Gradient Boosting - Recall: 0.9952\n",
            "Gradient Boosting - F1-score: 0.9891\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106258   1852]\n",
            " [   518 107591]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 109443\n",
            "Gradient Boosting - Training time: 0.26 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=8, K=3...\n",
            "Logistic Regression - Precision: 0.9707\n",
            "Logistic Regression - Recall: 0.9947\n",
            "Logistic Regression - F1-score: 0.9825\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[104863   3247]\n",
            " [   577 107532]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110779\n",
            "Logistic Regression - Training time: 0.01 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=8, K=3...\n",
            "Decision Tree - Precision: 0.9966\n",
            "Decision Tree - Recall: 0.9975\n",
            "Decision Tree - F1-score: 0.9970\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107737    373]\n",
            " [   266 107843]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108216\n",
            "Decision Tree - Training time: 0.02 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=8, K=5)...\n",
            "Epoch [1/30], Total Loss: 0.2883, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1308, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1205, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1119, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1071, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.1031, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.1014, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0979, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0949, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0950, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0924, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0923, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0919, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0886, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0886, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.0871, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.2516, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.0994, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0956, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0939, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0914, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0909, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0881, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0874, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.2975, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0941, LR: 0.050000\n",
            "Early stopping at epoch 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=8, K=5...\n",
            "Linear Regression - Precision: 0.9417\n",
            "Linear Regression - Recall: 0.9953\n",
            "Linear Regression - F1-score: 0.9677\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[101443   6667]\n",
            " [   513 107596]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 114263\n",
            "Linear Regression - Training time: 0.01 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=8, K=5...\n",
            "Gradient Boosting - Precision: 0.9707\n",
            "Gradient Boosting - Recall: 0.9958\n",
            "Gradient Boosting - F1-score: 0.9831\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[104863   3247]\n",
            " [   450 107659]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 110906\n",
            "Gradient Boosting - Training time: 0.25 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=8, K=5...\n",
            "Logistic Regression - Precision: 0.9456\n",
            "Logistic Regression - Recall: 0.9922\n",
            "Logistic Regression - F1-score: 0.9683\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[101933   6177]\n",
            " [   838 107271]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 113448\n",
            "Logistic Regression - Training time: 0.01 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=8, K=5...\n",
            "Decision Tree - Precision: 0.9932\n",
            "Decision Tree - Recall: 0.9943\n",
            "Decision Tree - F1-score: 0.9938\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107377    733]\n",
            " [   617 107492]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108225\n",
            "Decision Tree - Training time: 0.03 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=8, K=7)...\n",
            "Epoch [1/30], Total Loss: 0.3354, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1473, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1345, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1243, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1186, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.1128, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.1070, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.1021, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0974, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0939, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0924, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0907, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0906, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0898, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0874, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.0887, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.0878, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.5926, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0928, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0954, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0879, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0870, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0850, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0857, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0882, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0841, LR: 0.050000\n",
            "Epoch [27/30], Total Loss: 0.1901, LR: 0.050000\n",
            "Epoch [28/30], Total Loss: 0.0914, LR: 0.050000\n",
            "Epoch [29/30], Total Loss: 0.0909, LR: 0.050000\n",
            "Epoch [30/30], Total Loss: 0.0855, LR: 0.050000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=8, K=7...\n",
            "Linear Regression - Precision: 0.9752\n",
            "Linear Regression - Recall: 0.9962\n",
            "Linear Regression - F1-score: 0.9856\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105368   2742]\n",
            " [   415 107694]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 110436\n",
            "Linear Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=8, K=7...\n",
            "Gradient Boosting - Precision: 0.9896\n",
            "Gradient Boosting - Recall: 0.9970\n",
            "Gradient Boosting - F1-score: 0.9932\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106972   1138]\n",
            " [   328 107781]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 108919\n",
            "Gradient Boosting - Training time: 0.31 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=8, K=7...\n",
            "Logistic Regression - Precision: 0.9770\n",
            "Logistic Regression - Recall: 0.9941\n",
            "Logistic Regression - F1-score: 0.9855\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105578   2532]\n",
            " [   639 107470]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110002\n",
            "Logistic Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=8, K=7...\n",
            "Decision Tree - Precision: 0.9975\n",
            "Decision Tree - Recall: 0.9983\n",
            "Decision Tree - F1-score: 0.9979\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107839    271]\n",
            " [   185 107924]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108195\n",
            "Decision Tree - Training time: 0.02 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=8, K=11)...\n",
            "Epoch [1/30], Total Loss: 0.2975, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1315, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1179, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1111, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1070, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.1025, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0984, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0967, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0975, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0952, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0942, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0916, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0932, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0918, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.4205, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.1018, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.0995, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.1088, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.1033, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0913, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0918, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0887, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0866, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0852, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0834, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0833, LR: 0.050000\n",
            "Epoch [27/30], Total Loss: 0.2956, LR: 0.050000\n",
            "Epoch [28/30], Total Loss: 0.0892, LR: 0.050000\n",
            "Epoch [29/30], Total Loss: 0.0852, LR: 0.050000\n",
            "Epoch [30/30], Total Loss: 0.0862, LR: 0.050000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=8, K=11...\n",
            "Linear Regression - Precision: 0.9729\n",
            "Linear Regression - Recall: 0.9665\n",
            "Linear Regression - F1-score: 0.9697\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105201   2909]\n",
            " [  3625 104484]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 107393\n",
            "Linear Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=8, K=11...\n",
            "Gradient Boosting - Precision: 0.9863\n",
            "Gradient Boosting - Recall: 0.9964\n",
            "Gradient Boosting - F1-score: 0.9913\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106612   1498]\n",
            " [   390 107719]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 109217\n",
            "Gradient Boosting - Training time: 0.27 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=8, K=11...\n",
            "Logistic Regression - Precision: 0.9783\n",
            "Logistic Regression - Recall: 0.9954\n",
            "Logistic Regression - F1-score: 0.9868\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105719   2391]\n",
            " [   497 107612]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110003\n",
            "Logistic Regression - Training time: 0.02 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=8, K=11...\n",
            "Decision Tree - Precision: 0.9973\n",
            "Decision Tree - Recall: 0.9982\n",
            "Decision Tree - F1-score: 0.9978\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107817    293]\n",
            " [   193 107916]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108209\n",
            "Decision Tree - Training time: 0.02 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=16, K=3)...\n",
            "Epoch [1/30], Total Loss: 0.2382, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1234, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1114, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1056, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1021, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.0984, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0957, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0937, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0904, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0886, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0853, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0846, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0850, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0825, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0813, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.0792, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.4678, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.1030, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0920, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0897, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0905, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0853, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0821, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0806, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.1333, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0785, LR: 0.050000\n",
            "Epoch [27/30], Total Loss: 0.0782, LR: 0.050000\n",
            "Epoch [28/30], Total Loss: 0.0789, LR: 0.050000\n",
            "Epoch [29/30], Total Loss: 0.0836, LR: 0.050000\n",
            "Epoch [30/30], Total Loss: 0.0810, LR: 0.050000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=16, K=3...\n",
            "Linear Regression - Precision: 0.9741\n",
            "Linear Regression - Recall: 0.9944\n",
            "Linear Regression - F1-score: 0.9842\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105254   2856]\n",
            " [   605 107504]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 110360\n",
            "Linear Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=16, K=3...\n",
            "Gradient Boosting - Precision: 0.9866\n",
            "Gradient Boosting - Recall: 0.9968\n",
            "Gradient Boosting - F1-score: 0.9917\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106649   1461]\n",
            " [   346 107763]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 109224\n",
            "Gradient Boosting - Training time: 0.30 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=16, K=3...\n",
            "Logistic Regression - Precision: 0.9739\n",
            "Logistic Regression - Recall: 0.9958\n",
            "Logistic Regression - F1-score: 0.9847\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105223   2887]\n",
            " [   450 107659]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110546\n",
            "Logistic Regression - Training time: 0.02 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=16, K=3...\n",
            "Decision Tree - Precision: 0.9973\n",
            "Decision Tree - Recall: 0.9985\n",
            "Decision Tree - F1-score: 0.9979\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107815    295]\n",
            " [   162 107947]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108242\n",
            "Decision Tree - Training time: 0.03 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=16, K=5)...\n",
            "Epoch [1/30], Total Loss: 0.2329, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1185, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1104, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1052, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1014, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.0965, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0932, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0890, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0866, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0857, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0851, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0828, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0871, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0842, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.2920, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.0911, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.0840, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.0836, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0844, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0836, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0813, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0800, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0791, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0772, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.1658, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0869, LR: 0.050000\n",
            "Epoch [27/30], Total Loss: 0.0818, LR: 0.050000\n",
            "Epoch [28/30], Total Loss: 0.0829, LR: 0.050000\n",
            "Epoch [29/30], Total Loss: 0.0822, LR: 0.050000\n",
            "Epoch [30/30], Total Loss: 0.0778, LR: 0.050000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=16, K=5...\n",
            "Linear Regression - Precision: 0.9779\n",
            "Linear Regression - Recall: 0.9726\n",
            "Linear Regression - F1-score: 0.9753\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105738   2372]\n",
            " [  2961 105148]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 107520\n",
            "Linear Regression - Training time: 0.02 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=16, K=5...\n",
            "Gradient Boosting - Precision: 0.9898\n",
            "Gradient Boosting - Recall: 0.9967\n",
            "Gradient Boosting - F1-score: 0.9932\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[107005   1105]\n",
            " [   362 107747]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 108852\n",
            "Gradient Boosting - Training time: 0.30 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=16, K=5...\n",
            "Logistic Regression - Precision: 0.9773\n",
            "Logistic Regression - Recall: 0.9942\n",
            "Logistic Regression - F1-score: 0.9857\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105615   2495]\n",
            " [   630 107479]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 109974\n",
            "Logistic Regression - Training time: 0.01 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=16, K=5...\n",
            "Decision Tree - Precision: 0.9975\n",
            "Decision Tree - Recall: 0.9985\n",
            "Decision Tree - F1-score: 0.9980\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107839    271]\n",
            " [   165 107944]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108215\n",
            "Decision Tree - Training time: 0.02 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=16, K=7)...\n",
            "Epoch [1/30], Total Loss: 0.2640, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1272, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1170, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1073, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1027, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.1007, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0979, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0944, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0932, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0925, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0920, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0872, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0862, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0850, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0829, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.0850, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.5130, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.0998, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.1054, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0962, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.1098, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0873, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0883, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0830, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0842, LR: 0.050000\n",
            "Early stopping at epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=16, K=7...\n",
            "Linear Regression - Precision: 0.9635\n",
            "Linear Regression - Recall: 0.9062\n",
            "Linear Regression - F1-score: 0.9340\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[104396   3714]\n",
            " [ 10138  97971]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 101685\n",
            "Linear Regression - Training time: 0.01 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=16, K=7...\n",
            "Gradient Boosting - Precision: 0.9771\n",
            "Gradient Boosting - Recall: 0.9960\n",
            "Gradient Boosting - F1-score: 0.9864\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[105584   2526]\n",
            " [   436 107673]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 110199\n",
            "Gradient Boosting - Training time: 0.30 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=16, K=7...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression - Precision: 0.9653\n",
            "Logistic Regression - Recall: 0.9709\n",
            "Logistic Regression - F1-score: 0.9681\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[104336   3774]\n",
            " [  3150 104959]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 108733\n",
            "Logistic Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=16, K=7...\n",
            "Decision Tree - Precision: 0.9975\n",
            "Decision Tree - Recall: 0.9982\n",
            "Decision Tree - F1-score: 0.9978\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107836    274]\n",
            " [   193 107916]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108190\n",
            "Decision Tree - Training time: 0.02 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=16, K=11)...\n",
            "Epoch [1/30], Total Loss: 0.2533, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1202, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1124, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1072, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1015, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.0988, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0936, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0910, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0885, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0868, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0855, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0835, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0826, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0826, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0805, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.0807, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.9505, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.1242, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.1042, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0967, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0979, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0964, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0876, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0882, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0864, LR: 0.050000\n",
            "Early stopping at epoch 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=16, K=11...\n",
            "Linear Regression - Precision: 0.9734\n",
            "Linear Regression - Recall: 0.9957\n",
            "Linear Regression - F1-score: 0.9844\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105174   2936]\n",
            " [   468 107641]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 110577\n",
            "Linear Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=16, K=11...\n",
            "Gradient Boosting - Precision: 0.9868\n",
            "Gradient Boosting - Recall: 0.9965\n",
            "Gradient Boosting - F1-score: 0.9916\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106667   1443]\n",
            " [   376 107733]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 109176\n",
            "Gradient Boosting - Training time: 0.25 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=16, K=11...\n",
            "Logistic Regression - Precision: 0.9749\n",
            "Logistic Regression - Recall: 0.9952\n",
            "Logistic Regression - F1-score: 0.9849\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105336   2774]\n",
            " [   522 107587]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110361\n",
            "Logistic Regression - Training time: 0.02 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=16, K=11...\n",
            "Decision Tree - Precision: 0.9976\n",
            "Decision Tree - Recall: 0.9985\n",
            "Decision Tree - F1-score: 0.9980\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107847    263]\n",
            " [   160 107949]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108212\n",
            "Decision Tree - Training time: 0.04 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=32, K=3)...\n",
            "Epoch [1/30], Total Loss: 0.2383, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1174, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1080, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1022, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.0998, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.0950, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0900, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0872, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0858, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0845, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0834, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0831, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0813, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0821, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0797, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.2068, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.1023, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.0938, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0869, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0865, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0839, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0809, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0786, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0792, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0784, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0782, LR: 0.050000\n",
            "Epoch [27/30], Total Loss: 0.1925, LR: 0.050000\n",
            "Epoch [28/30], Total Loss: 0.0868, LR: 0.050000\n",
            "Epoch [29/30], Total Loss: 0.0858, LR: 0.050000\n",
            "Epoch [30/30], Total Loss: 0.0810, LR: 0.050000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=32, K=3...\n",
            "Linear Regression - Precision: 0.9750\n",
            "Linear Regression - Recall: 0.9954\n",
            "Linear Regression - F1-score: 0.9851\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105352   2758]\n",
            " [   492 107617]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 110375\n",
            "Linear Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=32, K=3...\n",
            "Gradient Boosting - Precision: 0.9874\n",
            "Gradient Boosting - Recall: 0.9956\n",
            "Gradient Boosting - F1-score: 0.9915\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106733   1377]\n",
            " [   478 107631]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 109008\n",
            "Gradient Boosting - Training time: 0.30 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=32, K=3...\n",
            "Logistic Regression - Precision: 0.9758\n",
            "Logistic Regression - Recall: 0.9957\n",
            "Logistic Regression - F1-score: 0.9856\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105438   2672]\n",
            " [   467 107642]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110314\n",
            "Logistic Regression - Training time: 0.03 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=32, K=3...\n",
            "Decision Tree - Precision: 0.9976\n",
            "Decision Tree - Recall: 0.9985\n",
            "Decision Tree - F1-score: 0.9981\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107854    256]\n",
            " [   159 107950]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108206\n",
            "Decision Tree - Training time: 0.03 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=32, K=5)...\n",
            "Epoch [1/30], Total Loss: 0.2685, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1266, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1098, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1037, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.1001, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.0976, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0949, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0920, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0877, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0854, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0834, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0813, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0818, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0794, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0789, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.0771, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.6178, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.1021, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0917, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0886, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0847, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0854, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0801, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0819, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0868, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0808, LR: 0.050000\n",
            "Early stopping at epoch 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=32, K=5...\n",
            "Linear Regression - Precision: 0.9723\n",
            "Linear Regression - Recall: 0.9931\n",
            "Linear Regression - F1-score: 0.9826\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105051   3059]\n",
            " [   746 107363]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 110422\n",
            "Linear Regression - Training time: 0.02 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=32, K=5...\n",
            "Gradient Boosting - Precision: 0.9888\n",
            "Gradient Boosting - Recall: 0.9968\n",
            "Gradient Boosting - F1-score: 0.9928\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106890   1220]\n",
            " [   342 107767]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 108987\n",
            "Gradient Boosting - Training time: 0.32 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=32, K=5...\n",
            "Logistic Regression - Precision: 0.9741\n",
            "Logistic Regression - Recall: 0.9969\n",
            "Logistic Regression - F1-score: 0.9854\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105246   2864]\n",
            " [   334 107775]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110639\n",
            "Logistic Regression - Training time: 0.02 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=32, K=5...\n",
            "Decision Tree - Precision: 0.9975\n",
            "Decision Tree - Recall: 0.9986\n",
            "Decision Tree - F1-score: 0.9981\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107841    269]\n",
            " [   147 107962]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108231\n",
            "Decision Tree - Training time: 0.02 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=32, K=7)...\n",
            "Epoch [1/30], Total Loss: 0.2384, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1190, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1116, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1054, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.0994, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.0958, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0928, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0885, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0880, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0875, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0845, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0825, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0809, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0821, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.2056, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.3476, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.1104, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.1060, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0967, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0986, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0903, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0864, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0831, LR: 0.050000\n",
            "Early stopping at epoch 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=32, K=7...\n",
            "Linear Regression - Precision: 0.9734\n",
            "Linear Regression - Recall: 0.9915\n",
            "Linear Regression - F1-score: 0.9823\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105178   2932]\n",
            " [   923 107186]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 110118\n",
            "Linear Regression - Training time: 0.01 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=32, K=7...\n",
            "Gradient Boosting - Precision: 0.9880\n",
            "Gradient Boosting - Recall: 0.9960\n",
            "Gradient Boosting - F1-score: 0.9919\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106799   1311]\n",
            " [   437 107672]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 108983\n",
            "Gradient Boosting - Training time: 0.33 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=32, K=7...\n",
            "Logistic Regression - Precision: 0.9754\n",
            "Logistic Regression - Recall: 0.9918\n",
            "Logistic Regression - F1-score: 0.9835\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105403   2707]\n",
            " [   891 107218]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 109925\n",
            "Logistic Regression - Training time: 0.02 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=32, K=7...\n",
            "Decision Tree - Precision: 0.9977\n",
            "Decision Tree - Recall: 0.9985\n",
            "Decision Tree - F1-score: 0.9981\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107858    252]\n",
            " [   166 107943]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108195\n",
            "Decision Tree - Training time: 0.04 seconds\n",
            "\n",
            "Training DAE with Soft Triple Loss (latent_dim=32, K=11)...\n",
            "Epoch [1/30], Total Loss: 0.2436, LR: 0.050000\n",
            "Epoch [2/30], Total Loss: 0.1196, LR: 0.050000\n",
            "Epoch [3/30], Total Loss: 0.1074, LR: 0.050000\n",
            "Epoch [4/30], Total Loss: 0.1046, LR: 0.050000\n",
            "Epoch [5/30], Total Loss: 0.0990, LR: 0.050000\n",
            "Epoch [6/30], Total Loss: 0.0944, LR: 0.050000\n",
            "Epoch [7/30], Total Loss: 0.0901, LR: 0.050000\n",
            "Epoch [8/30], Total Loss: 0.0876, LR: 0.050000\n",
            "Epoch [9/30], Total Loss: 0.0858, LR: 0.050000\n",
            "Epoch [10/30], Total Loss: 0.0848, LR: 0.050000\n",
            "Epoch [11/30], Total Loss: 0.0843, LR: 0.050000\n",
            "Epoch [12/30], Total Loss: 0.0818, LR: 0.050000\n",
            "Epoch [13/30], Total Loss: 0.0814, LR: 0.050000\n",
            "Epoch [14/30], Total Loss: 0.0803, LR: 0.050000\n",
            "Epoch [15/30], Total Loss: 0.0792, LR: 0.050000\n",
            "Epoch [16/30], Total Loss: 0.8430, LR: 0.050000\n",
            "Epoch [17/30], Total Loss: 0.0981, LR: 0.050000\n",
            "Epoch [18/30], Total Loss: 0.0899, LR: 0.050000\n",
            "Epoch [19/30], Total Loss: 0.0889, LR: 0.050000\n",
            "Epoch [20/30], Total Loss: 0.0844, LR: 0.050000\n",
            "Epoch [21/30], Total Loss: 0.0814, LR: 0.050000\n",
            "Epoch [22/30], Total Loss: 0.0797, LR: 0.050000\n",
            "Epoch [23/30], Total Loss: 0.0780, LR: 0.050000\n",
            "Epoch [24/30], Total Loss: 0.0762, LR: 0.050000\n",
            "Epoch [25/30], Total Loss: 0.0762, LR: 0.050000\n",
            "Epoch [26/30], Total Loss: 0.0760, LR: 0.050000\n",
            "Epoch [27/30], Total Loss: 0.3450, LR: 0.050000\n",
            "Epoch [28/30], Total Loss: 0.0847, LR: 0.050000\n",
            "Epoch [29/30], Total Loss: 0.0794, LR: 0.050000\n",
            "Epoch [30/30], Total Loss: 0.0768, LR: 0.050000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_14584\\231037438.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  DAE_soft_triplet.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Linear Regression with latent_dim=32, K=11...\n",
            "Linear Regression - Precision: 0.9747\n",
            "Linear Regression - Recall: 0.9966\n",
            "Linear Regression - F1-score: 0.9855\n",
            "Linear Regression - Confusion Matrix:\n",
            "[[105308   2802]\n",
            " [   364 107745]]\n",
            "Linear Regression - Số lượng bất thường phát hiện: 110547\n",
            "Linear Regression - Training time: 0.00 seconds\n",
            "\n",
            "Evaluating Gradient Boosting with latent_dim=32, K=11...\n",
            "Gradient Boosting - Precision: 0.9882\n",
            "Gradient Boosting - Recall: 0.9970\n",
            "Gradient Boosting - F1-score: 0.9926\n",
            "Gradient Boosting - Confusion Matrix:\n",
            "[[106820   1290]\n",
            " [   326 107783]]\n",
            "Gradient Boosting - Số lượng bất thường phát hiện: 109073\n",
            "Gradient Boosting - Training time: 0.30 seconds\n",
            "\n",
            "Evaluating Logistic Regression with latent_dim=32, K=11...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\belon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression - Precision: 0.9753\n",
            "Logistic Regression - Recall: 0.9963\n",
            "Logistic Regression - F1-score: 0.9857\n",
            "Logistic Regression - Confusion Matrix:\n",
            "[[105386   2724]\n",
            " [   402 107707]]\n",
            "Logistic Regression - Số lượng bất thường phát hiện: 110431\n",
            "Logistic Regression - Training time: 0.03 seconds\n",
            "\n",
            "Evaluating Decision Tree with latent_dim=32, K=11...\n",
            "Decision Tree - Precision: 0.9977\n",
            "Decision Tree - Recall: 0.9986\n",
            "Decision Tree - F1-score: 0.9982\n",
            "Decision Tree - Confusion Matrix:\n",
            "[[107865    245]\n",
            " [   153 107956]]\n",
            "Decision Tree - Số lượng bất thường phát hiện: 108201\n",
            "Decision Tree - Training time: 0.02 seconds\n",
            "# Kết quả Grid Search cho Linear Regression\n",
            "\n",
            "| Latent Dim | K | Precision | Recall | F1-score | Thời gian huấn luyện DAE (s) | Thời gian huấn luyện Model (s) |\n",
            "|------------|---|-----------|--------|----------|-----------------------------|-------------------------------|\n",
            "| 8 | 3 | 0.9668 | 0.9954 | 0.9809 | 1604.09 | 0.12 |\n",
            "| 8 | 5 | 0.9417 | 0.9953 | 0.9677 | 1676.76 | 0.09 |\n",
            "| 8 | 7 | 0.9752 | 0.9962 | 0.9856 | 1926.35 | 0.10 |\n",
            "| 8 | 11 | 0.9729 | 0.9665 | 0.9697 | 1887.20 | 0.10 |\n",
            "| 16 | 3 | 0.9741 | 0.9944 | 0.9842 | 1909.10 | 0.21 |\n",
            "| 16 | 5 | 0.9779 | 0.9726 | 0.9753 | 1892.68 | 0.22 |\n",
            "| 16 | 7 | 0.9635 | 0.9062 | 0.9340 | 1619.01 | 0.24 |\n",
            "| 16 | 11 | 0.9734 | 0.9957 | 0.9844 | 1607.43 | 0.28 |\n",
            "| 32 | 3 | 0.9750 | 0.9954 | 0.9851 | 1905.59 | 0.63 |\n",
            "| 32 | 5 | 0.9723 | 0.9931 | 0.9826 | 1685.16 | 0.63 |\n",
            "| 32 | 7 | 0.9734 | 0.9915 | 0.9823 | 1547.60 | 0.69 |\n",
            "| 32 | 11 | 0.9747 | 0.9966 | 0.9855 | 1931.38 | 0.63 |\n",
            "\n",
            "# Kết quả Grid Search cho Gradient Boosting\n",
            "\n",
            "| Latent Dim | K | Precision | Recall | F1-score | Thời gian huấn luyện DAE (s) | Thời gian huấn luyện Model (s) |\n",
            "|------------|---|-----------|--------|----------|-----------------------------|-------------------------------|\n",
            "| 8 | 3 | 0.9831 | 0.9952 | 0.9891 | 1604.09 | 223.03 |\n",
            "| 8 | 5 | 0.9707 | 0.9958 | 0.9831 | 1676.76 | 286.15 |\n",
            "| 8 | 7 | 0.9896 | 0.9970 | 0.9932 | 1926.35 | 252.76 |\n",
            "| 8 | 11 | 0.9863 | 0.9964 | 0.9913 | 1887.20 | 260.62 |\n",
            "| 16 | 3 | 0.9866 | 0.9968 | 0.9917 | 1909.10 | 479.85 |\n",
            "| 16 | 5 | 0.9898 | 0.9967 | 0.9932 | 1892.68 | 422.37 |\n",
            "| 16 | 7 | 0.9771 | 0.9960 | 0.9864 | 1619.01 | 536.05 |\n",
            "| 16 | 11 | 0.9868 | 0.9965 | 0.9916 | 1607.43 | 395.77 |\n",
            "| 32 | 3 | 0.9874 | 0.9956 | 0.9915 | 1905.59 | 836.02 |\n",
            "| 32 | 5 | 0.9888 | 0.9968 | 0.9928 | 1685.16 | 837.93 |\n",
            "| 32 | 7 | 0.9880 | 0.9960 | 0.9919 | 1547.60 | 755.64 |\n",
            "| 32 | 11 | 0.9882 | 0.9970 | 0.9926 | 1931.38 | 790.96 |\n",
            "\n",
            "# Kết quả Grid Search cho Logistic Regression\n",
            "\n",
            "| Latent Dim | K | Precision | Recall | F1-score | Thời gian huấn luyện DAE (s) | Thời gian huấn luyện Model (s) |\n",
            "|------------|---|-----------|--------|----------|-----------------------------|-------------------------------|\n",
            "| 8 | 3 | 0.9707 | 0.9947 | 0.9825 | 1604.09 | 1.70 |\n",
            "| 8 | 5 | 0.9456 | 0.9922 | 0.9683 | 1676.76 | 1.27 |\n",
            "| 8 | 7 | 0.9770 | 0.9941 | 0.9855 | 1926.35 | 3.01 |\n",
            "| 8 | 11 | 0.9783 | 0.9954 | 0.9868 | 1887.20 | 1.24 |\n",
            "| 16 | 3 | 0.9739 | 0.9958 | 0.9847 | 1909.10 | 39.04 |\n",
            "| 16 | 5 | 0.9773 | 0.9942 | 0.9857 | 1892.68 | 12.27 |\n",
            "| 16 | 7 | 0.9653 | 0.9709 | 0.9681 | 1619.01 | 51.20 |\n",
            "| 16 | 11 | 0.9749 | 0.9952 | 0.9849 | 1607.43 | 10.52 |\n",
            "| 32 | 3 | 0.9758 | 0.9957 | 0.9856 | 1905.59 | 46.16 |\n",
            "| 32 | 5 | 0.9741 | 0.9969 | 0.9854 | 1685.16 | 19.83 |\n",
            "| 32 | 7 | 0.9754 | 0.9918 | 0.9835 | 1547.60 | 41.45 |\n",
            "| 32 | 11 | 0.9753 | 0.9963 | 0.9857 | 1931.38 | 49.50 |\n",
            "\n",
            "# Kết quả Grid Search cho Decision Tree\n",
            "\n",
            "| Latent Dim | K | Precision | Recall | F1-score | Thời gian huấn luyện DAE (s) | Thời gian huấn luyện Model (s) |\n",
            "|------------|---|-----------|--------|----------|-----------------------------|-------------------------------|\n",
            "| 8 | 3 | 0.9966 | 0.9975 | 0.9970 | 1604.09 | 9.76 |\n",
            "| 8 | 5 | 0.9932 | 0.9943 | 0.9938 | 1676.76 | 13.05 |\n",
            "| 8 | 7 | 0.9975 | 0.9983 | 0.9979 | 1926.35 | 12.65 |\n",
            "| 8 | 11 | 0.9973 | 0.9982 | 0.9978 | 1887.20 | 10.57 |\n",
            "| 16 | 3 | 0.9973 | 0.9985 | 0.9979 | 1909.10 | 20.27 |\n",
            "| 16 | 5 | 0.9975 | 0.9985 | 0.9980 | 1892.68 | 21.20 |\n",
            "| 16 | 7 | 0.9975 | 0.9982 | 0.9978 | 1619.01 | 27.60 |\n",
            "| 16 | 11 | 0.9976 | 0.9985 | 0.9980 | 1607.43 | 19.51 |\n",
            "| 32 | 3 | 0.9976 | 0.9985 | 0.9981 | 1905.59 | 44.15 |\n",
            "| 32 | 5 | 0.9975 | 0.9986 | 0.9981 | 1685.16 | 36.94 |\n",
            "| 32 | 7 | 0.9977 | 0.9985 | 0.9981 | 1547.60 | 53.41 |\n",
            "| 32 | 11 | 0.9977 | 0.9986 | 0.9982 | 1931.38 | 48.83 |\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results_df = main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
