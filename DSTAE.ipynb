{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Triple Loss with Deep Autoencoder\n",
    "\n",
    "This notebook implements a Deep Autoencoder (DAE) combined with Soft Triple Loss for anomaly detection on the `combine.csv` dataset. The Soft Triple Loss is adapted from a TensorFlow implementation, optimized with vectorization and L2 normalization. The code also includes other variants (TDAE, DMAE, DAE+PCA) and evaluates performance using traditional machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels_to_numeric(labels):\n",
    "    label_mapping = {'BENIGN': 0}\n",
    "    numeric_labels = labels.apply(lambda x: 0 if 'BENIGN' in str(x) else 1)\n",
    "    return numeric_labels\n",
    "\n",
    "def preprocess_data(data_path, features):\n",
    "    data = pd.read_csv(data_path, low_memory=False)\n",
    "    if ' Label' not in data.columns:\n",
    "        raise ValueError(\"Cột 'Label' không tồn tại trong dữ liệu.\")\n",
    "    missing_features = [f for f in features if f not in data.columns]\n",
    "    if missing_features:\n",
    "        features = [f for f in features if f in data.columns]\n",
    "    if len(features) == 0:\n",
    "        raise ValueError(\"Không có đặc trưng nào hợp lệ trong dữ liệu.\")\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna()\n",
    "    numeric_labels = map_labels_to_numeric(data[' Label'])\n",
    "    benign_data = data[data[' Label'].str.contains('BENIGN', case=False, na=False)][features]\n",
    "    all_data = data[features]\n",
    "    if benign_data.empty:\n",
    "        raise ValueError(\"Không tìm thấy dữ liệu hợp lệ với nhãn 'Benign'.\")\n",
    "    non_numeric_cols = benign_data.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "    if len(non_numeric_cols) > 0:\n",
    "        benign_data = benign_data.drop(columns=non_numeric_cols)\n",
    "        all_data = all_data.drop(columns=non_numeric_cols)\n",
    "    benign_data = benign_data.loc[:, benign_data.var() > 1e-3]\n",
    "    all_data = all_data[benign_data.columns]\n",
    "    benign_data = benign_data.dropna()\n",
    "    all_data = all_data.dropna()\n",
    "    numeric_labels = numeric_labels[all_data.index]\n",
    "    if benign_data.empty:\n",
    "        raise ValueError(\"DataFrame rỗng sau khi loại bỏ NaN.\")\n",
    "    selected_features = benign_data.columns.tolist()\n",
    "    scaler = MinMaxScaler()\n",
    "    benign_data_scaled = scaler.fit_transform(benign_data)\n",
    "    all_data_scaled = scaler.transform(all_data)\n",
    "    train_data, test_data = train_test_split(benign_data_scaled, test_size=0.2, random_state=42)\n",
    "    rus = RandomUnderSampler(sampling_strategy=1.0, random_state=42)\n",
    "    all_data_resampled, test_labels_resampled = rus.fit_resample(all_data_scaled, numeric_labels)\n",
    "    return train_data, test_data, all_data_resampled, test_labels_resampled, scaler, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[512, 256, 128, 16]):\n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.SELU(),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        decoder_layers = []\n",
    "        hidden_dims = hidden_dims[::-1]\n",
    "        for dim in hidden_dims[:-1]:\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, dim),\n",
    "                nn.SELU(),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_dim = dim\n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Triple Loss (Adapted from TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_triple_loss(encoded, labels, centers, gamma=0.1, delta=0.01, lambda_param=2.0):\n",
    "    if len(encoded) == 0 or centers is None:\n",
    "        return torch.tensor(0.0, device=encoded.device)\n",
    "    \n",
    "    K = 5  # Number of centers per class\n",
    "    num_classes = 2  # Benign (0) and Anomalous (1)\n",
    "    d = encoded.shape[1]\n",
    "    \n",
    "    # L2 normalization\n",
    "    encoded_norm = F.normalize(encoded, p=2, dim=1)  # [batch_size, d]\n",
    "    centers_norm = F.normalize(centers, p=2, dim=2)  # [num_classes, K, d]\n",
    "    \n",
    "    # Inner logits: [batch_size, num_classes, K]\n",
    "    inner_logits = torch.einsum('bd,nkd->bnk', encoded_norm, centers_norm)\n",
    "    \n",
    "    # Inner softmax: [batch_size, num_classes, K]\n",
    "    inner_softmax = F.softmax(inner_logits / gamma, dim=2)\n",
    "    \n",
    "    # Similarity S: [batch_size, num_classes]\n",
    "    S = lambda_param * (torch.sum(inner_softmax * inner_logits, dim=2) - delta * labels)\n",
    "    \n",
    "    # Outer softmax: [batch_size, num_classes]\n",
    "    outer_softmax = F.softmax(S, dim=1)\n",
    "    \n",
    "    # Loss: [batch_size]\n",
    "    loss = -torch.sum(torch.log(torch.sum(outer_softmax * labels, dim=1) + 1e-6))\n",
    "    \n",
    "    return loss / encoded.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centers(d=16, num_classes=2, K=5):\n",
    "    return torch.randn(num_classes, K, d, requires_grad=True)\n",
    "\n",
    "def triplet_loss(encoded, labels, margin=1.0):\n",
    "    benign = encoded[labels == 0]\n",
    "    anomalous = encoded[labels == 1]\n",
    "    if len(benign) < 2 or len(anomalous) < 1:\n",
    "        return torch.tensor(0.0).to(encoded.device)\n",
    "    anchor = benign\n",
    "    positive = benign[torch.randperm(len(benign))[:len(anchor)]]\n",
    "    negative = anomalous[torch.randint(0, len(anomalous), (len(anchor),))]\n",
    "    distance_positive = F.pairwise_distance(anchor, positive)\n",
    "    distance_negative = F.pairwise_distance(anchor, negative)\n",
    "    losses = F.relu(distance_positive - distance_negative + margin)\n",
    "    return losses.mean()\n",
    "\n",
    "def expand_dims(var, dim=0):\n",
    "    sizes = list(var.size())\n",
    "    sizes.insert(dim, 1)\n",
    "    return var.view(*sizes)\n",
    "\n",
    "def comparison_mask(a_labels, b_labels):\n",
    "    return torch.eq(expand_dims(a_labels, 1), expand_dims(b_labels, 0)).float()\n",
    "\n",
    "def dynamic_partition(X, partitions, n_clusters):\n",
    "    indices = [torch.where(partitions == i)[0] for i in range(n_clusters)]\n",
    "    return [X[idx] for idx in indices if len(idx) > 0]\n",
    "\n",
    "def compute_euclidean_distance(x, y):\n",
    "    return torch.sum((x - y)**2, dim=2)\n",
    "\n",
    "def magnet_loss(encoded, labels, num_clusters=8, m=8, alpha=1.0, epsilon=1e-6, min_variance=1.0, max_dist=100.0, offset=1e-4):\n",
    "    device = encoded.device\n",
    "    batch_size = encoded.size(0)\n",
    "    if batch_size < 2 or len(torch.unique(labels)) < 2:\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    if torch.isnan(encoded).any() or torch.isinf(encoded).any():\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    variance = max(torch.var(encoded, dim=0).mean().item(), min_variance) + epsilon\n",
    "    var_normalizer = -1 / (2 * variance**2)\n",
    "    benign = encoded[labels == 0]\n",
    "    attack = encoded[labels == 1]\n",
    "    if len(benign) < m or len(attack) < m:\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    num_clusters = min(num_clusters, len(benign) // m, len(attack) // m)\n",
    "    if num_clusters < 1:\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    benign_indices = torch.randperm(len(benign), device=device)[:num_clusters * m]\n",
    "    attack_indices = torch.randperm(len(attack), device=device)[:num_clusters * m]\n",
    "    clusters = torch.arange(0, num_clusters, device=device).repeat_interleave(m)\n",
    "    benign_clusters = benign[benign_indices]\n",
    "    attack_clusters = attack[attack_indices]\n",
    "    cluster_classes = torch.cat([torch.zeros(num_clusters, dtype=torch.long, device=device),\n",
    "                                torch.ones(num_clusters, dtype=torch.long, device=device)])\n",
    "    all_clusters = torch.cat([benign_clusters, attack_clusters], dim=0)\n",
    "    cluster_examples = dynamic_partition(all_clusters, clusters, num_clusters)\n",
    "    cluster_means = []\n",
    "    valid_cluster_classes = []\n",
    "    for i, x in enumerate(cluster_examples):\n",
    "        if x.size(0) > 0:\n",
    "            cluster_means.append(torch.mean(x, dim=0))\n",
    "            valid_cluster_classes.append(cluster_classes[i])\n",
    "    if not cluster_means:\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    cluster_means = torch.stack(cluster_means)\n",
    "    valid_cluster_classes = torch.tensor(valid_cluster_classes, device=device)\n",
    "    sample_costs = compute_euclidean_distance(cluster_means, expand_dims(encoded, 1))\n",
    "    sample_costs = torch.clamp(sample_costs, max=max_dist)\n",
    "    intra_cluster_mask = comparison_mask(labels.type(torch.float), valid_cluster_classes.type(torch.float))\n",
    "    intra_cluster_costs = torch.sum(intra_cluster_mask * sample_costs, dim=1)\n",
    "    numerator = torch.exp(var_normalizer * intra_cluster_costs - alpha)\n",
    "    diff_class_mask = 1 - comparison_mask(labels.type(torch.float), valid_cluster_classes.type(torch.float))\n",
    "    denom_sample_costs = torch.exp(var_normalizer * sample_costs)\n",
    "    denominator = torch.sum(diff_class_mask * denom_sample_costs, dim=1) + offset\n",
    "    if torch.isnan(numerator).any() or torch.isnan(denominator).any() or (denominator <= epsilon).any():\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    losses = F.relu(-torch.log(numerator / (denominator + epsilon) + epsilon))\n",
    "    total_loss = torch.mean(losses)\n",
    "    if torch.isnan(total_loss):\n",
    "        return torch.tensor(0.0).to(device)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DAE_soft_triplet(model, train_loader, optimizer, scheduler, centers, epochs=100, device='cuda', patience=10, gamma=0.1, delta=0.01, lambda_param=2.0):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels_one_hot = F.one_hot(labels, num_classes=2).float()\n",
    "            output, encoded = model(data)\n",
    "            recon_loss = nn.MSELoss()(data, output)\n",
    "            soft_triple = soft_triple_loss(encoded, labels_one_hot, centers, gamma, delta, lambda_param)\n",
    "            total_loss = recon_loss + soft_triple\n",
    "            if torch.isnan(total_loss):\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "            batch_count += 1\n",
    "        if batch_count == 0:\n",
    "            continue\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        if not torch.isnan(torch.tensor(avg_loss)) and avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_DAE_soft_triplet_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    return losses\n",
    "\n",
    "def train_dae(model, train_loader, optimizer, scheduler, epochs=100, device='cuda', patience=10):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            output, _ = model(data)\n",
    "            recon_loss = nn.MSELoss()(data, output)\n",
    "            optimizer.zero_grad()\n",
    "            recon_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += recon_loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Reconstruction Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_dae_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    return losses\n",
    "\n",
    "def train_TDAE(model, train_loader, optimizer, scheduler, epochs=100, device='cuda', patience=10):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            output, encoded = model(data)\n",
    "            recon_loss = nn.MSELoss()(data, output)\n",
    "            triplet = triplet_loss(encoded, labels)\n",
    "            total_loss = recon_loss + triplet\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_TDAE_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    return losses\n",
    "\n",
    "def train_DMAE(model, train_loader, optimizer, scheduler, epochs=100, device='cuda', patience=20, alpha1=0.9, alpha2=0.01):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            output, encoded = model(data)\n",
    "            recon_loss = nn.MSELoss()(data, output)\n",
    "            if torch.isnan(encoded).any() or torch.isinf(encoded).any():\n",
    "                total_loss = alpha1 * recon_loss\n",
    "            else:\n",
    "                magnet = magnet_loss(encoded, labels, num_clusters=8, m=8, alpha=1.0)\n",
    "                if torch.isnan(magnet):\n",
    "                    total_loss = alpha1 * recon_loss\n",
    "                else:\n",
    "                    total_loss = alpha1 * recon_loss + alpha2 * magnet\n",
    "            if torch.isnan(total_loss):\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            epoch_loss += total_loss.item()\n",
    "            batch_count += 1\n",
    "        if batch_count == 0:\n",
    "            continue\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        losses.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Total Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n",
    "        if not torch.isnan(torch.tensor(avg_loss)) and avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_DMAE_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(model, data, device='cuda'):\n",
    "    model.eval()\n",
    "    data_tensor = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, encoded = model(data_tensor)\n",
    "    return encoded.cpu().numpy()\n",
    "\n",
    "def evaluate_model(model, model_name, data, labels):\n",
    "    start_time = time.time()\n",
    "    if model_name == 'Linear Regression':\n",
    "        predictions_proba = model.predict(data)\n",
    "        predictions = (predictions_proba >= 0.5).astype(int)\n",
    "    else:\n",
    "        predictions = model.predict(data)\n",
    "    training_time = time.time() - start_time\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    precision = precision_score(labels, predictions, zero_division=0)\n",
    "    recall = recall_score(labels, predictions, zero_division=0)\n",
    "    f1 = f1_score(labels, predictions, zero_division=0)\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}\")\n",
    "    print(f\"{model_name} - Recall: {recall:.4f}\")\n",
    "    print(f\"{model_name} - F1-score: {f1:.4f}\")\n",
    "    print(f\"{model_name} - Confusion Matrix:\\n{cm}\")\n",
    "    print(f\"{model_name} - Số lượng bất thường phát hiện: {np.sum(predictions)}\")\n",
    "    print(f\"{model_name} - Training time: {training_time:.2f} seconds\")\n",
    "    return precision, recall, f1, training_time, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belon\\AppData\\Local\\Temp\\ipykernel_16376\\3116177242.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training DAE with Soft Triple Loss...\n",
      "Epoch [1/100], Total Loss: 0.2627, LR: 0.050000\n",
      "Epoch [2/100], Total Loss: 0.1237, LR: 0.050000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 202\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 202\u001b[0m     results_df \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     28\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 29\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_DAE_soft_triplet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDAE_soft_triplet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m soft_triplet_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m, in \u001b[0;36mtrain_DAE_soft_triplet\u001b[1;34m(model, train_loader, optimizer, scheduler, centers, epochs, device, patience, gamma, delta, lambda_param)\u001b[0m\n\u001b[0;32m      7\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      8\u001b[0m batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels_one_hot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\connection.py:896\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    893\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    894\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 896\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\multiprocessing\\connection.py:828\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    826\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 828\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_path = r'C:\\Users\\belon\\Downloads\\combine.csv\\combine.csv'\n",
    "    df1 = pd.read_csv(data_path)\n",
    "    features = df1.columns.drop([' Label', ' Destination Port'])\n",
    "    train_data, test_data, all_data_resampled, test_labels_resampled, scaler, selected_features = preprocess_data(data_path, features)\n",
    "    if len(train_data) == 0 or len(test_data) == 0:\n",
    "        raise ValueError(\"Dữ liệu huấn luyện hoặc kiểm tra rỗng.\")\n",
    "    input_dim = len(selected_features)\n",
    "    all_data_tensor = torch.tensor(all_data_resampled, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(test_labels_resampled.values, dtype=torch.long)\n",
    "    dataset_all = torch.utils.data.TensorDataset(all_data_tensor, labels_tensor)\n",
    "    train_loader_all = torch.utils.data.DataLoader(dataset_all, batch_size=2048, shuffle=True, num_workers=4)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "    results = []\n",
    "\n",
    "    # Training DAE with Soft Triple Loss\n",
    "    print(\"\\nTraining DAE with Soft Triple Loss...\")\n",
    "    DAE_soft_triplet = DeepAutoencoder(input_dim=input_dim).to(device)\n",
    "    centers = initialize_centers(d=16, num_classes=2, K=5).to(device)\n",
    "    optimizer = torch.optim.Adam(list(DAE_soft_triplet.parameters()) + [centers], lr=0.05)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    start_time = time.time()\n",
    "    losses = train_DAE_soft_triplet(DAE_soft_triplet, train_loader_all, optimizer, scheduler, centers, epochs=100, device=device, gamma=0.1, delta=0.01, lambda_param=2.0)\n",
    "    soft_triplet_time = time.time() - start_time\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.title('DAE with Soft Triple Loss Training Loss (16 Dimensions)')\n",
    "    plt.show()\n",
    "    if os.path.exists('best_DAE_soft_triplet_model.pth'):\n",
    "        DAE_soft_triplet.load_state_dict(torch.load('best_DAE_soft_triplet_model.pth', map_location=device))\n",
    "    soft_triplet_features = extract_features(DAE_soft_triplet, all_data_resampled, device)\n",
    "    train_features_soft_triplet, test_features_soft_triplet, train_labels_soft_triplet, test_labels_soft_triplet = train_test_split(\n",
    "        soft_triplet_features, test_labels_resampled, test_size=0.2, random_state=42, stratify=test_labels_resampled\n",
    "    )\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name} with DAE+Soft Triple features...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(train_features_soft_triplet, train_labels_soft_triplet)\n",
    "        training_time = time.time() - start_time\n",
    "        precision, recall, f1, _, cm = evaluate_model(model, model_name, test_features_soft_triplet, test_labels_soft_triplet)\n",
    "        results.append({\n",
    "            'Method': 'DAE+Soft Triple',\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Training Time (s)': training_time\n",
    "        })\n",
    "\n",
    "    # Training TDAE\n",
    "    print(\"\\nTraining TDAE...\")\n",
    "    TDAE = DeepAutoencoder(input_dim=input_dim).to(device)\n",
    "    optimizer_TDAE = torch.optim.Adam(TDAE.parameters(), lr=0.05)\n",
    "    scheduler_TDAE = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_TDAE, mode='min', factor=0.5, patience=10)\n",
    "    start_time = time.time()\n",
    "    losses_TDAE = train_TDAE(TDAE, train_loader_all, optimizer_TDAE, scheduler_TDAE, epochs=100, device=device)\n",
    "    TDAE_time = time.time() - start_time\n",
    "    plt.plot(losses_TDAE)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.title('TDAE Training Loss (16 Dimensions)')\n",
    "    plt.show()\n",
    "    if os.path.exists('best_TDAE_model.pth'):\n",
    "        TDAE.load_state_dict(torch.load('best_TDAE_model.pth', map_location=device))\n",
    "    TDAE_features = extract_features(TDAE, all_data_resampled, device)\n",
    "    train_features_TDAE, test_features_TDAE, train_labels_TDAE, test_labels_TDAE = train_test_split(\n",
    "        TDAE_features, test_labels_resampled, test_size=0.2, random_state=42, stratify=test_labels_resampled\n",
    "    )\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name} with TDAE features...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(train_features_TDAE, train_labels_TDAE)\n",
    "        training_time = time.time() - start_time\n",
    "        precision, recall, f1, _, cm = evaluate_model(model, model_name, test_features_TDAE, test_labels_TDAE)\n",
    "        results.append({\n",
    "            'Method': 'TDAE',\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Training Time (s)': training_time\n",
    "        })\n",
    "\n",
    "    # Training DAE\n",
    "    print(\"\\nTraining DAE...\")\n",
    "    dae = DeepAutoencoder(input_dim=input_dim).to(device)\n",
    "    optimizer_dae = torch.optim.Adam(dae.parameters(), lr=0.05)\n",
    "    scheduler_dae = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_dae, mode='min', factor=0.5, patience=10)\n",
    "    start_time = time.time()\n",
    "    losses_dae = train_dae(dae, train_loader_all, optimizer_dae, scheduler_dae, epochs=30, device=device)\n",
    "    dae_time = time.time() - start_time\n",
    "    plt.plot(losses_dae)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reconstruction Loss')\n",
    "    plt.title('DAE Training Loss (16 Dimensions)')\n",
    "    plt.show()\n",
    "    if os.path.exists('best_dae_model.pth'):\n",
    "        dae.load_state_dict(torch.load('best_dae_model.pth', map_location=device))\n",
    "    dae_features = extract_features(dae, all_data_resampled, device)\n",
    "    train_features_dae, test_features_dae, train_labels_dae, test_labels_dae = train_test_split(\n",
    "        dae_features, test_labels_resampled, test_size=0.2, random_state=42, stratify=test_labels_resampled\n",
    "    )\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name} with DAE features...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(train_features_dae, train_labels_dae)\n",
    "        training_time = time.time() - start_time\n",
    "        precision, recall, f1, _, cm = evaluate_model(model, model_name, test_features_dae, test_labels_dae)\n",
    "        results.append({\n",
    "            'Method': 'DAE',\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Training Time (s)': training_time\n",
    "        })\n",
    "\n",
    "    print(\"\\nApplying PCA to DAE features...\")\n",
    "    pca = PCA(n_components=16, random_state=42)\n",
    "    dae_pca_features = pca.fit_transform(dae_features)\n",
    "    train_features_pca, test_features_pca, train_labels_pca, test_labels_pca = train_test_split(\n",
    "        dae_pca_features, test_labels_resampled, test_size=0.2, random_state=42, stratify=test_labels_resampled\n",
    "    )\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name} with DAE+PCA features...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(train_features_pca, train_labels_pca)\n",
    "        training_time = time.time() - start_time\n",
    "        precision, recall, f1, _, cm = evaluate_model(model, model_name, test_features_pca, test_labels_pca)\n",
    "        results.append({\n",
    "            'Method': 'DAE+PCA',\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Training Time (s)': training_time\n",
    "        })\n",
    "\n",
    "    # Training DMAE\n",
    "    print(\"\\nTraining DMAE...\")\n",
    "    DMAE = DeepAutoencoder(input_dim=input_dim).to(device)\n",
    "    optimizer_DMAE = torch.optim.Adam(DMAE.parameters(), lr=0.05, betas=(0.9, 0.999))\n",
    "    scheduler_DMAE = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_DMAE, mode='min', factor=0.5, patience=10)\n",
    "    start_time = time.time()\n",
    "    losses_DMAE = train_DMAE(DMAE, train_loader_all, optimizer_DMAE, scheduler_DMAE, epochs=100, device=device, alpha1=0.9, alpha2=0.01)\n",
    "    DMAE_time = time.time() - start_time\n",
    "    plt.plot(losses_DMAE)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.title('DMAE Training Loss')\n",
    "    plt.show()\n",
    "    if os.path.exists('best_DMAE_model.pth'):\n",
    "        DMAE.load_state_dict(torch.load('best_DMAE_model.pth', map_location=device))\n",
    "    DMAE_features = extract_features(DMAE, all_data_resampled, device)\n",
    "    train_features_DMAE, test_features_DMAE, train_labels_DMAE, test_labels_DMAE = train_test_split(\n",
    "        DMAE_features, test_labels_resampled, test_size=0.2, random_state=42, stratify=test_labels_resampled\n",
    "    )\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name} with DMAE features...\")\n",
    "        start_time = time.time()\n",
    "        model.fit(train_features_DMAE, train_labels_DMAE)\n",
    "        training_time = time.time() - start_time\n",
    "        precision, recall, f1, _, cm = evaluate_model(model, model_name, test_features_DMAE, test_labels_DMAE)\n",
    "        results.append({\n",
    "            'Method': 'DMAE',\n",
    "            'Model': model_name,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1,\n",
    "            'Training Time (s)': training_time\n",
    "        })\n",
    "\n",
    "    torch.save(dae.state_dict(), 'dae_model_16dims.pth')\n",
    "    torch.save(TDAE.state_dict(), 'TDAE_model_16dims.pth')\n",
    "    torch.save(DAE_soft_triplet .state_dict(), 'DAE_soft_triplet_model_16dims.pth')\n",
    "    torch.save(DMAE.state_dict(), 'DMAE_model_16dims.pth')\n",
    "    print(f\"Thời gian huấn luyện DAE: {dae_time:.2f} giây\")\n",
    "    print(f\"Thời gian huấn luyện TDAE: {TDAE_time:.2f} giây\")\n",
    "    print(f\"Thời gian huấn luyện DAE+Soft Triple: {soft_triplet_time:.2f} giây\")\n",
    "    print(f\"Thời gian huấn luyện DMAE: {DMAE_time:.2f} giây\")\n",
    "    results_df = pd.DataFrame(results)\n",
    "    for model_name in models.keys():\n",
    "        model_results = results_df[results_df['Model'] == model_name]\n",
    "        markdown_table = f\"# So sánh hiệu suất của {model_name}\\n\\n\"\n",
    "        markdown_table += \"| Phương pháp | Precision | Recall | F1-score | Thời gian huấn luyện (s) |\\n\"\n",
    "        markdown_table += \"|------------|-----------|--------|----------|-------------------------|\\n\"\n",
    "        for _, row in model_results.iterrows():\n",
    "            markdown_table += f\"| {row['Method']} | {row['Precision']:.4f} | {row['Recall']:.4f} | {row['F1-score']:.4f} | {row['Training Time (s)']:.2f} |\\n\"\n",
    "        print(markdown_table)\n",
    "        with open(f'comparison_results_{model_name.lower().replace(\" \", \"_\")}.md', 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_table)\n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
